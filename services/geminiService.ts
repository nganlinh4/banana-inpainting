
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Generates an edit for a specific image region.
 * 
 * @param imageBase64 The base64 string of the cropped image region (PNG/JPEG)
 * @param prompt The user's text prompt
 * @param referenceImages Optional array of base64 strings for reference
 * @param maskBase64 Optional base64 string of the mask (black/white or transparent)
 * @returns The base64 string of the generated image
 */
export const generateRegionEdit = async (
  imageBase64: string,
  prompt: string,
  referenceImages: string[] = [],
  maskBase64?: string
): Promise<string> => {
  try {
    // Remove data URL prefix if present for clean base64
    const cleanBase64 = imageBase64.replace(/^data:image\/(png|jpeg|jpg);base64,/, '');

    let finalPrompt = prompt;
    if (maskBase64) {
        // If a mask is provided, we guide the model to use it.
        finalPrompt = `${prompt} (Modify primarily the area defined by the mask)`;
    }

    const parts: any[] = [
        {
          inlineData: {
            mimeType: 'image/png',
            data: cleanBase64,
          },
        },
        {
          text: finalPrompt,
        },
    ];

    // Add Mask if present
    if (maskBase64) {
        const cleanMask = maskBase64.replace(/^data:image\/(png|jpeg|jpg);base64,/, '');
        parts.push({
            inlineData: {
                mimeType: 'image/png',
                data: cleanMask
            }
        });
    }

    // Add reference images to parts
    referenceImages.forEach((ref) => {
        const cleanRef = ref.replace(/^data:image\/(png|jpeg|jpg|webp);base64,/, '');
        parts.push({
            inlineData: {
                mimeType: 'image/png', // Defaulting to png, API is flexible
                data: cleanRef
            }
        });
    });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: parts,
      },
    });

    // Parse response to find the image part
    if (response.candidates && response.candidates[0].content && response.candidates[0].content.parts) {
        for (const part of response.candidates[0].content.parts) {
            if (part.inlineData && part.inlineData.data) {
                return part.inlineData.data;
            }
        }
    }

    // If no image found, construct a helpful error message from text parts or finish reason
    let errorMessage = "No image generated by Gemini.";
    const candidate = response.candidates?.[0];

    if (candidate) {
        const textParts = candidate.content?.parts?.filter(p => p.text).map(p => p.text).join(' ');
        if (textParts) {
            console.log("Gemini Response (Text-only):", textParts);
            // We do not show the raw text because it can be confusing (e.g. "Okay I can do that") when no image is actually returned.
            errorMessage = "Generation failed: The model responded with text instead of an image. Please try rephrasing your prompt.";
        } else if (candidate.finishReason) {
            errorMessage = `Generation stopped: ${candidate.finishReason}. Please try again.`;
        }
    }

    throw new Error(errorMessage);

  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};

/**
 * Transcribes audio to text using Groq Whisper API.
 * @param audioBase64 Base64 string of the audio file
 * @param apiKey Groq API Key
 * @returns Transcribed text
 */
export const transcribeAudio = async (audioBase64: string, apiKey: string): Promise<string> => {
  try {
    if (!apiKey) throw new Error("Groq API Key is missing");

    // Convert Base64 to Blob
    const fetchResponse = await fetch(audioBase64);
    const blob = await fetchResponse.blob();

    // Prepare FormData for Groq API
    const formData = new FormData();
    // Filename 'audio.webm' or 'audio.m4a' is important for the API to detect type, 
    // although blob type usually handles it.
    formData.append('file', blob, 'audio.webm'); 
    formData.append('model', 'whisper-large-v3-turbo');
    formData.append('temperature', '0');

    const response = await fetch("https://api.groq.com/openai/v1/audio/transcriptions", {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${apiKey}`
      },
      body: formData
    });

    if (!response.ok) {
        const errData = await response.json().catch(() => ({}));
        throw new Error(errData.error?.message || `Groq API Error: ${response.statusText}`);
    }

    const data = await response.json();
    return data.text?.trim() || '';

  } catch (error) {
    console.error("Transcription Error:", error);
    throw error;
  }
};